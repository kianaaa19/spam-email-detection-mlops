{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3W6-HZVx19l"
   },
   "source": [
    "# Install library requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d56rgjv-l5Fs",
    "outputId": "de5f68c1-535f-4c92-a8da-22d6f8576e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tfx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n",
      "Requirement already satisfied: ml-pipelines-sdk==1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.0)\n",
      "Requirement already satisfied: ml-metadata<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
      "Requirement already satisfied: packaging<21,>=20 in /usr/local/lib/python3.10/dist-packages (from tfx) (20.9)\n",
      "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\n",
      "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.20.3)\n",
      "Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.4.4)\n",
      "Requirement already satisfied: google-apitools<1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.5.31)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.12.11)\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.5.0)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.47 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.50.0)\n",
      "Requirement already satisfied: attrs<22,>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (21.4.0)\n",
      "Requirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\n",
      "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.11.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.46.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3,>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.34.4)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.62.1)\n",
      "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.7)\n",
      "Requirement already satisfied: kubernetes<13,>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (12.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.24.3)\n",
      "Requirement already satisfied: pyarrow<11,>=10 in /usr/local/lib/python3.10/dist-packages (from tfx) (10.0.1)\n",
      "Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.1)\n",
      "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-hub<0.14,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.13.0)\n",
      "Requirement already satisfied: tensorflow-data-validation<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.46.0,>=0.45.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.45.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-transform<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
      "Requirement already satisfied: tfx-bsl<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.7)\n",
      "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.10.0)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.9.4)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.19)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.6.1)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (4.6.3)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.23.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2023.4)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2023.12.25)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.31.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (5.3.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.1.1)\n",
      "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.15.2)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.21.1)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.9.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.44.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.16.0)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.13.3)\n",
      "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.13.3)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.7.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.10.10)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.63.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.0.3)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.10.15)\n",
      "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (0.16)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (2.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.1.5)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (2.13.1)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (1.0.5)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2024.2.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (67.7.2)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging<21,>=20->tfx) (3.1.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.36.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.3.2)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.5.3)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-metadata<1.15,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.14.0)\n",
      "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (7.34.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (7.7.1)\n",
      "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (9.4.0)\n",
      "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.11.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tfx) (0.43.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.48.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tfx) (0.13.0)\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tfx) (7.7.0)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.4)\n",
      "Requirement already satisfied: deprecated>=1.2.14 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (1.2.14)\n",
      "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.15.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx) (1.5.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (0.6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.19.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.0.43)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.16.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.9.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.5.6)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.6.6)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.0.10)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.6.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tfx) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tfx) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tfx) (3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (3.0.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.3.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.13)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.5.5)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (23.2.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (23.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.7.2)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.10.3)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.5.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.20.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.0.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.2.0)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.24.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.4)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.9.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.12.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.3.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.17.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.20.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.7.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.5)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.22)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.3)\n",
      "install: missing destination file operand after 'docker'\n",
      "Try 'install --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "!pip install tfx\n",
    "!pip install opencv-python\n",
    "!pip install -q kaggle\n",
    "!sudo install docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jtkdR7oETG4"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z-V3vHYGJKX"
   },
   "source": [
    "# WRITE FILE: COMPONENTS.PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Mknmt82qeQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O54EkrtcGXag"
   },
   "outputs": [],
   "source": [
    "COMPONENTS = \"spam_components.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1j2B3ITeEoR8",
    "outputId": "c6598764-3df4-4b65-f9d0-d6c6902d655d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spam_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {COMPONENTS}\n",
    "\"\"\"\n",
    "spam_components.py\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            SchemaGen, StatisticsGen, Trainer, Transform,\n",
    "                            Tuner)\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import \\\n",
    "    LatestBlessedModelStrategy\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "\n",
    "def init_components(\n",
    "    data_dir,\n",
    "    transform_module,\n",
    "    trainer_module,\n",
    "    tuner_module,\n",
    "    train_steps,\n",
    "    eval_steps,\n",
    "    serving_model_dir,\n",
    "):\n",
    "    \"\"\"Initiate tfx pipeline components\n",
    "\n",
    "    Args:\n",
    "        args (dict): args that containts some dependencies\n",
    "\n",
    "    Returns:\n",
    "        tuple: TFX pipeline components\n",
    "    \"\"\"\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    example_gen = CsvExampleGen(\n",
    "        input_base=data_dir,\n",
    "        output_config=output,\n",
    "    )\n",
    "\n",
    "    statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "    )\n",
    "\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "    )\n",
    "\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "    )\n",
    "\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        module_file=os.path.abspath(transform_module),\n",
    "    )\n",
    "\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(tuner_module),\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=[\"train\"],\n",
    "            num_steps=train_steps,\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=eval_steps,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        module_file=trainer_module,\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        hyperparameters=tuner.outputs[\"best_hyperparameters\"],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=[\"train\"],\n",
    "            num_steps=train_steps,\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=eval_steps\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing),\n",
    "    ).with_id(\"Latest_blessed_model_resolve\")\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key=\"label\")],\n",
    "        slicing_specs=[\n",
    "            tfma.SlicingSpec(),\n",
    "            tfma.SlicingSpec(feature_keys=[\"text\"]),\n",
    "        ],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name=\"AUC\"),\n",
    "                tfma.MetricConfig(class_name=\"Precision\"),\n",
    "                tfma.MetricConfig(class_name=\"Recall\"),\n",
    "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "                tfma.MetricConfig(class_name=\"TruePositives\"),\n",
    "                tfma.MetricConfig(class_name=\"FalsePositives\"),\n",
    "                tfma.MetricConfig(class_name=\"TrueNegatives\"),\n",
    "                tfma.MetricConfig(class_name=\"FalseNegatives\"),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name=\"BinaryAccuracy\",\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={\"value\": .6},\n",
    "                        ),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={\"value\": 1e-4},\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            ]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        baseline_model=model_resolver.outputs[\"model\"],\n",
    "        eval_config=eval_config,\n",
    "    )\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUvO5QPHHhyN"
   },
   "source": [
    "# WRITE FILE: PIPELINE.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AwNpKfPlJS3d"
   },
   "outputs": [],
   "source": [
    "PIPELINE = \"spam_pipeline.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqxQ87tbHopM",
    "outputId": "903e938a-2eb7-46e3-b68c-f7799f16ed61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spam_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PIPELINE}\n",
    "\"\"\"\n",
    "spam_pipeline.py\n",
    "\"\"\"\n",
    "from typing import Text\n",
    "\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "\n",
    "\n",
    "def init_pipeline(pipeline_root: Text, pipeline_name, metadata_path, components):\n",
    "    \"\"\"Initiate tfx pipeline\n",
    "\n",
    "    Args:\n",
    "        pipeline_root (Text): a path to th pipeline directory\n",
    "        pipeline_name (str): pipeline name\n",
    "        metadata_path (str): a path to the metadata directory\n",
    "        components (dict): tfx components\n",
    "\n",
    "    Returns:\n",
    "        pipeline.Pipeline: pipeline orchestration\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
    "\n",
    "    beam_args = [\n",
    "        \"--direct_running_mode=multi_processing\",\n",
    "        \"----direct_num_workers=0\",\n",
    "    ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path,\n",
    "        ),\n",
    "        eam_pipeline_args=beam_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbjTv6PsJ_Ir"
   },
   "source": [
    "# WRITE FILE: TRANSFORM.PY\n",
    "Processing yang dilakukan hanya untuk merubah nama feature dan label yang telah di transform menjadi label_xf, text_xf, dan transform feature kedalam format lowercase string dan untuk casting label kedalam format int64 untuk memastikan tipe data pada label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KCSjc8vjKCwq"
   },
   "outputs": [],
   "source": [
    "TRANSFORM_MODULE_FILE = \"spam_transform.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jceavmqHKD1X",
    "outputId": "4d0033c8-81b6-4f16-bf20-937baf4bc434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spam_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\"\"\"\n",
    "spam_transform.py\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "LABEL_KEY = \"label\"\n",
    "FEATURE_KEY = \"text\"\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features\n",
    "\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw features.\n",
    "\n",
    "    Return:\n",
    "        outputs: map from feature keys to transformed features.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
    "\n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0EhbcucKNd1"
   },
   "source": [
    "# WRITE FILE: TUNER.PY\n",
    "Melakukan tuning hyperparameter pada arsitektur model yang digunakan untuk mendapatkan hyperparameter terbaik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SbvXGsTtKQ9j"
   },
   "outputs": [],
   "source": [
    "TUNER_FILE = \"tuner_module_file.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xj6W6VaZKVMe",
    "outputId": "482c1b78-257d-4951-b1fd-176b6b9c23e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tuner_module_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TUNER_FILE}\n",
    "\"\"\"\n",
    "tuner_module_file.py\n",
    "\"\"\"\n",
    "from typing import NamedTuple, Dict, Text, Any\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras_tuner.engine import base_tuner\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "\n",
    "\n",
    "LABEL_KEY = \"label\"\n",
    "FEATURE_KEY = \"text\"\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "TunerFnResult = NamedTuple(\"TunerFnResult\", [\n",
    "    (\"tuner\", base_tuner.BaseTuner),\n",
    "    (\"fit_kwargs\", Dict[Text, Any]),\n",
    "])\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    return f\"{key}_xf\"\n",
    "\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=8):\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key=transformed_name(LABEL_KEY),\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def model_builder(hp, vectorizer_layer):\n",
    "    num_hidden_layers = hp.Choice(\n",
    "        \"num_hidden_layers\", values=[1, 2]\n",
    "    )\n",
    "    embed_dims = hp.Int(\n",
    "        \"embed_dims\", min_value=16, max_value=128, step=32\n",
    "    )\n",
    "    lstm_units= hp.Int(\n",
    "        \"lstm_units\", min_value=32, max_value=128, step=32\n",
    "    )\n",
    "    dense_units = hp.Int(\n",
    "        \"dense_units\", min_value=32, max_value=256, step=32\n",
    "    )\n",
    "    dropout_rate = hp.Float(\n",
    "        \"dropout_rate\", min_value=0.1, max_value=0.5, step=0.1\n",
    "    )\n",
    "    learning_rate = hp.Choice(\n",
    "        \"learning_rate\", values=[1e-2, 1e-3, 1e-4]\n",
    "    )\n",
    "\n",
    "    inputs = tf.keras.Input(\n",
    "        shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string\n",
    "    )\n",
    "\n",
    "    x = vectorizer_layer(inputs)\n",
    "    x = tf.keras.layers.Embedding(input_dim=5000, output_dim=embed_dims)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units))(x)\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        x = tf.keras.layers.Dense(dense_units, activation=tf.nn.relu)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs):\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    train_set = input_fn(\n",
    "        fn_args.train_files[0], tf_transform_output, NUM_EPOCHS\n",
    "    )\n",
    "    eval_set = input_fn(\n",
    "        fn_args.eval_files[0], tf_transform_output, NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    vectorizer_dataset = train_set.map(\n",
    "        lambda f, l: f[transformed_name(FEATURE_KEY)]\n",
    "    )\n",
    "\n",
    "    vectorizer_layer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=5000,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=500,\n",
    "    )\n",
    "    vectorizer_layer.adapt(vectorizer_dataset)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=lambda hp: model_builder(hp, vectorizer_layer),\n",
    "        objective=kt.Objective('binary_accuracy', direction='max'),\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        factor=3,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name=\"kt_hyperband\",\n",
    "    )\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"callbacks\": [early_stopping_callback],\n",
    "            \"x\": train_set,\n",
    "            \"validation_data\": eval_set,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHH_MpzCJBAQ"
   },
   "source": [
    "# WRITE FILE: TRAINER.PY\n",
    "Melatih model dengan menggunakan hyperparameter yang telah dituning sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3f2N9ZxaJPN3"
   },
   "outputs": [],
   "source": [
    "TRAINER = \"spam_trainer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0DsR8t5Jj0N",
    "outputId": "0cad2876-c8a7-452f-ac37-7586a9512e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spam_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER}\n",
    "\"\"\"\n",
    "spam_trainer.py\n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_hub as hub\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "\n",
    "LABEL_KEY = \"label\"\n",
    "FEATURE_KEY = \"text\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compressed data\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "\n",
    "def input_fn(file_pattern,\n",
    "             tf_transform_output,\n",
    "             num_epochs,\n",
    "             batch_size=64)->tf.data.Dataset:\n",
    "    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n",
    "\n",
    "    # Get post_transform feature spec\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "    # create batches of data\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key = transformed_name(LABEL_KEY))\n",
    "    return dataset\n",
    "\n",
    "# os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'\n",
    "# embed = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 100\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "EMBEDDING_DIM=16\n",
    "def model_builder():\n",
    "    \"\"\"Build machine learning model\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
    "    reshaped_narrative = tf.reshape(inputs, [-1])\n",
    "    x = vectorize_layer(reshaped_narrative)\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, name=\"embedding\")(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "    )\n",
    "\n",
    "    # print(model)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "        # get predictions using the transformed features\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir, update_freq='batch'\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "    # Load the transform output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Create batches of data\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
    "    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
    "    vectorize_layer.adapt(\n",
    "        [j[0].numpy()[0] for j in [\n",
    "            i[0][transformed_name(FEATURE_KEY)]\n",
    "                for i in list(train_set)]])\n",
    "\n",
    "    # Build the model\n",
    "    model = model_builder()\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x = train_set,\n",
    "            validation_data = val_set,\n",
    "            callbacks = [tensorboard_callback, es, mc],\n",
    "            steps_per_epoch = 1000,\n",
    "            validation_steps= 1000,\n",
    "            epochs=10)\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "                                    tf.TensorSpec(\n",
    "                                    shape=[None],\n",
    "                                    dtype=tf.string,\n",
    "                                    name='examples'))\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxdTZvmeLR7w"
   },
   "source": [
    "# WRITE FILE: DOCKERFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oRkjkqPDLIoA"
   },
   "outputs": [],
   "source": [
    "DOCKER = \"Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qA4I9_4vLXPf",
    "outputId": "f141bba5-230b-4550-bdf8-59e9e3d80e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DOCKER}\n",
    "FROM tensorflow/serving:latest\n",
    "\n",
    "COPY ./serving_model_dir /models\n",
    "ENV MODEL_NAME=spam-detection-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dJtijsLLdho"
   },
   "source": [
    "# WRITE FILE: REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_0cDCBTdLq-X"
   },
   "outputs": [],
   "source": [
    "REQ = \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUo5QknLL0oX",
    "outputId": "7866d2ba-2725-4518-a97e-a0f014d0e895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REQ}\n",
    "# Install\n",
    "!pip install tfx\n",
    "!pip install opencv-python\n",
    "!pip install -q kaggle\n",
    "!sudo install docker\n",
    "\n",
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from google.colab import files\n",
    "from google.colab.patches import cv2_imshow\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dense, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D, LSTM, Flatten\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "import tensorflow_hub as hub\n",
    "from tfx.components import Tuner\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import Pusher\n",
    "from tfx.proto import pusher_pb2\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Conda:\n",
    "pip install pandas numpy opencv-python tensorflow matplotlib scikit-learn nltk tensorflow-hub tensorflow-model-analysis tensorflow-transform\n",
    "pip install tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "B_DL2o9GLmDX"
   },
   "outputs": [],
   "source": [
    "REQTF = \"requirements-tfserving.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7CsxmDvLmmW",
    "outputId": "a1bbcab1-9f6a-4512-c64f-c02e0d84df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements-tfserving.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REQTF}\n",
    "tensorflow-serving-api\n",
    "transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEpkVUULK86e"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oild0RJDA1sV"
   },
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c1s8Wn_GAzMl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from modules.spam_pipeline import init_local_pipeline\n",
    "from modules.spam_components import init_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEZb1QqGA-HC"
   },
   "source": [
    "# SET VARIABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Idt7ZsiGA7Zw"
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'kianaa19-pipeline'\n",
    "\n",
    "DATA_ROOT = 'data'\n",
    "TRANSFORM_MODULE_FILE = 'modules/spam_transform.py'\n",
    "TUNER_MODULE_FILE = 'modules/tuner_module_file.py'\n",
    "TRAINER_MODULE_FILE = 'modules/spam_trainer.py'\n",
    "\n",
    "OUTPUT_BASE = 'output'\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snuC1ptmBWQW"
   },
   "source": [
    "# RUN PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wSFH-Q8BLxv",
    "outputId": "491a1e14-b558-46ce-8ba1-b1dce1efb191"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "num_hidden_layers (Choice)\n",
      "{'default': 1, 'conditions': [], 'values': [1, 2], 'ordered': True}\n",
      "embed_dims (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "lstm_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |num_hidden_layers\n",
      "48                |48                |embed_dims\n",
      "128               |128               |lstm_units\n",
      "224               |224               |dense_units\n",
      "0.1               |0.1               |dropout_rate\n",
      "0.01              |0.01              |learning_rate\n",
      "1                 |1                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "0                 |0                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "    492/Unknown - 281s 561ms/step - loss: 0.2230 - binary_accuracy: 0.9395"
     ]
    }
   ],
   "source": [
    "from modules.spam_pipeline1 import init_pipeline\n",
    "from modules.spam_components import init_components\n",
    "\n",
    "components = init_components(\n",
    "    data_dir=DATA_ROOT,\n",
    "    transform_module=TRANSFORM_MODULE_FILE,\n",
    "    trainer_module=TRAINER_MODULE_FILE,\n",
    "    tuner_module=TUNER_MODULE_FILE,\n",
    "    train_steps=1000,\n",
    "    eval_steps=800,\n",
    "    serving_model_dir=serving_model_dir,\n",
    ")\n",
    "\n",
    "pipeline = init_pipeline(\n",
    "    pipeline_root,\n",
    "    PIPELINE_NAME,\n",
    "    metadata_path,\n",
    "    components\n",
    ")\n",
    "BeamDagRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_7i4xdRMhkA"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFN9FW8ZMlP3"
   },
   "source": [
    "# PYLINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqLjjIJJMqIm",
    "outputId": "9e79ed25-d723-44f6-a8a0-96ed55f50116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pylint\n",
      "  Downloading pylint-3.1.0-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.6/515.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pylint) (4.2.0)\n",
      "Collecting astroid<=3.2.0-dev0,>=3.1.0 (from pylint)\n",
      "  Downloading astroid-3.1.0-py3-none-any.whl (275 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.6/275.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting isort!=5.13.0,<6,>=4.2.5 (from pylint)\n",
      "  Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mccabe<0.8,>=0.6 (from pylint)\n",
      "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting tomlkit>=0.10.1 (from pylint)\n",
      "  Downloading tomlkit-0.12.4-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: dill>=0.2 in /usr/local/lib/python3.10/dist-packages (from pylint) (0.3.1.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pylint) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from astroid<=3.2.0-dev0,>=3.1.0->pylint) (4.5.0)\n",
      "Installing collected packages: tomlkit, mccabe, isort, astroid, pylint\n",
      "Successfully installed astroid-3.1.0 isort-5.13.2 mccabe-0.7.0 pylint-3.1.0 tomlkit-0.12.4\n",
      "Collecting autopep8\n",
      "  Downloading autopep8-2.1.0-py2.py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m845.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycodestyle>=2.11.0 (from autopep8)\n",
      "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8) (2.0.1)\n",
      "Installing collected packages: pycodestyle, autopep8\n",
      "Successfully installed autopep8-2.1.0 pycodestyle-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pylint\n",
    "!pip install autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9StaZdzqNQcd",
    "outputId": "403bc286-86a4-45b2-a7fc-5b61f7b9ccad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Module spam_components\n",
      "spam_components.py:100:0: R1707: Disallow trailing comma tuple (trailing-comma-tuple)\n",
      "spam_components.py:28:0: R0913: Too many arguments (6/5) (too-many-arguments)\n",
      "spam_components.py:28:0: R0914: Too many local variables (20/15) (too-many-locals)\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Your code has been rated at 8.85/10 (previous run: 8.15/10, +0.70)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_components.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA-bW2LJOYJO"
   },
   "outputs": [],
   "source": [
    "!autopep8 --in-place --aggressive --aggressive spam_components.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBtN9csmOXW2",
    "outputId": "0e91d40b-207b-452a-c53e-1813dc3617c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------\n",
      "Your code has been rated at 10.00/10 (previous run: 8.85/10, +1.15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_components.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isaaF3EDOF_j",
    "outputId": "9a7c2304-4ae6-4c57-f05a-a61736c113a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Module spam_pipeline\n",
      "spam_pipeline.py:24:4: W0621: Redefining name 'components' from outer scope (line 60) (redefined-outer-name)\n",
      "spam_pipeline.py:24:16: W0621: Redefining name 'pipeline_root' from outer scope (line 19) (redefined-outer-name)\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Your code has been rated at 9.13/10 (previous run: 9.13/10, +0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3liC8dWOvjd"
   },
   "outputs": [],
   "source": [
    "!autopep8 --in-place --aggressive --aggressive spam_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajvDsg92O2Kf",
    "outputId": "f3a0ad0e-a325-458e-bbd5-6b5a446cf284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------\n",
      "Your code has been rated at 10.00/10 (previous run: 9.13/10, +0.87)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPbI6H2iOIrZ",
    "outputId": "bc9c39da-22f5-4793-a9ad-0adb9745eb3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Module spam_transform\n",
      "spam_transform.py:1:0: C0114: Missing module docstring (missing-module-docstring)\n",
      "\n",
      "-----------------------------------\n",
      "Your code has been rated at 9.00/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2_GpUZdPBT1"
   },
   "outputs": [],
   "source": [
    "!autopep8 --in-place --aggressive --aggressive spam_transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_cMG0AnO5BL",
    "outputId": "aab273bd-8410-4be8-e9d4-907b29794096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------\n",
      "Your code has been rated at 10.00/10 (previous run: 9.00/10, +1.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Twz2hnjFOMw2",
    "outputId": "238a185f-d6cd-438d-fb06-5957f7eeeab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Module tuner_module_file\n",
      "tuner_module_file.py:29:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "tuner_module_file.py:33:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "tuner_module_file.py:37:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "tuner_module_file.py:54:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "tuner_module_file.py:99:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Your code has been rated at 8.91/10 (previous run: 5.53/10, +3.38)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint tuner_module_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc9d7515PFml"
   },
   "outputs": [],
   "source": [
    "!autopep8 --in-place --aggressive --aggressive tuner_module_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UunMAGMjO6RV",
    "outputId": "c06159f5-fc63-4237-9e23-f80b1d361e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------\n",
      "Your code has been rated at 10.00/10 (previous run: 8.91/10, +1.09)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint tuner_module_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RihIkOAZOPfK",
    "outputId": "f5c4e19d-9fd3-4466-a430-56b54ea78cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Module spam_trainer\n",
      "spam_trainer.py:112:0: C0301: Line too long (108/100) (line-too-long)\n",
      "spam_trainer.py:113:0: C0301: Line too long (145/100) (line-too-long)\n",
      "spam_trainer.py:6:0: E0401: Unable to import 'tensorflow.keras' (import-error)\n",
      "spam_trainer.py:57:0: C0103: Constant name \"embedding_dim\" doesn't conform to UPPER_CASE naming style (invalid-name)\n",
      "spam_trainer.py:104:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "spam_trainer.py:7:0: C0411: standard import \"os\" should be placed before third party imports \"tensorflow\", \"tensorflow_transform\", \"tensorflow.keras.layers\" (wrong-import-order)\n",
      "spam_trainer.py:8:0: W0611: Unused tensorflow_hub imported as hub (unused-import)\n",
      "\n",
      "-----------------------------------\n",
      "Your code has been rated at 8.00/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCBy--cyPGu0"
   },
   "outputs": [],
   "source": [
    "!autopep8 --in-place --aggressive --aggressive spam_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmE4yyeRO7qc",
    "outputId": "1a6aeeb1-339c-4b53-c7ca-ac55bce50dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------\n",
      "Your code has been rated at 10.00/10 (previous run: 8.00/10, +2.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint spam_trainer.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
